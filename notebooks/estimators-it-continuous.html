<html>
<head>
  <title>Notebooks : /estimators-it-continuous.html</title>
</head>
<body>
<h1>Notebooks</h1>

<p>
[
<a href="http://ddarmon.github.io/master/notebooks/">Home</a>
]
</p>

<h2>Fri, 29 Sep 2017</h2>
<h3><a name="estimators-it-continuous">Estimators for Information Theoretic Properties of Continuous-valued Stochastic Processes</a></h3>
<div class="blosxomStory">
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<ul>
<li>
<p>Recommended:</p>
<ul>
<li>
<p>Kernel Density Estimators:</p>
<ul>
<li>
<p>H. Liu, L. Wasserman, and J. D. Lafferty, "Exponential concentration for mutual information estimation with application to forests," <a href="http://papers.nips.cc/paper/4768-exponential-concentration-for-mutual-information-estimation-with-application-to-forests.pdf">Neural Information Processing Systems (NIPS)</a>, 2012.</p>
</li>
<li>
<p>K. Kandasamy, A. Krishnamurthy, B. Poczos, L. Wasserman, and J. M. robins, "Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations," <a href="http://arxiv.org/abs/1411.4342v3">arXiv.org</a>. 16-Nov-2014.</p>
</li>
<li>
<p>K. Kandasamy, A. Krishnamurthy, and B. Poczos, "Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations," <a href="http://papers.nips.cc/paper/5911-nonparametric-von-mises-estimators-for-entropies-divergences-and-mutual-informations">Neural Information Processing Systems (NIPS)</a>, 2015.</p>
</li>
</ul>
</li>
<li>
<p>Nearest Neighbor Estimators:</p>
<ul>
<li>
<p>A. Kraskov, H. Stogbauer, and P. Grassberger, "Estimating mutual information," Phys. Rev. E, vol. 69, no. 6, p. 066138, Jun. 2004. <a href="http://arxiv.org/abs/cond-mat/0305641">arXiv</a></p>
</li>
<li>
<p>B. Poczos and J. G. Schneider, "Nonparametric estimation of conditional information and divergences," International Conference on Artificial Intelligence and Statistics, 2012, <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_PoczosS12.pdf">preprint</a>.</p>
</li>
<li>
<p>B. Poczos and J. G. Schneider, ''On the Estimation of alpha-Divergences.,'' AISTATS, 2011, <a href="http://www.jmlr.org/proceedings/papers/v15/poczos11a/poczos11a.pdf">link</a>.</p>
</li>
<li>
<p>S. Gao, G. Ver Steeg, and A. Galstyan, "Efficient Estimation of Mutual Information for Strongly Dependent Variables," <a href="http://arxiv.org/abs/1411.2003v3">arXiv.org</a>, vol. cs.IT. 07-Nov-2014.</p>
</li>
<li>
<p>S. Gao, G. Ver Steeg, and A. Galstyan, "Estimating Mutual Information by Local Gaussian Approximation," <a href="http://arxiv.org/abs/1508.00536v2">arXiv.org</a>, vol. cs.IT. 03-Aug-2015.</p>
</li>
<li>
<p>D. Lombardi and S. Pant, "Nonparametric k-nearest-neighbor entropy estimator," Phys. Rev. E, vol. 93, no. 1, p. 013310, Jan. 2016. <a href="http://arxiv.org/abs/1506.06501">arXiv</a></p>
</li>
<li>
<p>T. B. Berrett, R. J. Samworth, and M. Yuan, "Efficient multivariate entropy estimation via $k$-nearest neighbour distances." 01-Jun-2016. <a href="https://arxiv.org/abs/1606.00304">arXiv</a></p>
</li>
<li>
<p>S. Singh and B. Poczos, "Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional Estimators." <a href="https://arxiv.org/abs/1606.01554">arXiv</a></p>
</li>
<li>
<p>W. Gao, S. Oh, and P. Viswanath, "Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation." <a href="http://arxiv.org/abs/1609.02208v1">arXiv</a></p>
</li>
<li>
<p>S. Delattre and N. Fournier, "On the Kozachenko–Leonenko entropy estimator," Journal of Statistical Planning and Inference, vol. 185, pp. 69–93, 2017.</p>
<ul>
<li>Establishes the asymptotic normality of the $k = 1$ $k$th-nearest neighbor entropy estimator, and provides an explicit form for the sampling distribution in low dimensional cases.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>
  posted at: 10:19 |
  path: <a href="http://ddarmon.github.io/master/notebooks/" title="path">/</a> |
  <a href="http://ddarmon.github.io/master/notebooks/estimators-it-continuous.html">permanent link to this entry</a>
</p>
</div>

<p>
  <a href="http://pyblosxom.github.com/"><img src="http://pyblosxom.github.com/images/pb_pyblosxom.gif" alt="Made with Pyblosxom" border="0" /></a>
</p>
</body>
</html>
