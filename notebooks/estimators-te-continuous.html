<html>
<head>
  <title>Notebooks : /estimators-te-continuous.html</title>
</head>
<body>
<h1>Notebooks</h1>

<p>
[
<a href="http://ddarmon.github.io/master/notebooks/">Home</a>
]
</p>

<h2>Wed, 19 Oct 2016</h2>
<h3><a name="estimators-te-continuous">Estimators of Transfer Entropy for Continuous-valued Time Series</a></h3>
<div class="blosxomStory">
<p>A collection of the papers that are used to define the various estimators of transfer entropy that have become popular in the neuroscientific community.</p>
<ul>
<li>
<p>G. G'omez-Herrero, W. Wu, K. Rutanen, M. Soriano, G. Pipa, and R. Vicente, "Assessing Coupling Dynamics from an Ensemble of Time Series," Entropy, vol. 17, no. 4, pp. 1958-1970, Apr. 2015.</p>
<ul>
<li>This paper gives the more-or-less standard "KSG version'' of the transfer entropy estimator as a special case of an estimator for what they call 'entropy combinations,' i.e. linear combinations of joint and marginal entropies. This class thus includes finite-order entropy rates, mutual information, transfer entropy, etc., and their conditional ilk. They really focus on doing this in an <em>ensemble</em> fashion, as one might in a trial-based neuroscientific setting. This is, in essence, the method as implemented in JIDT. Overall, their time-local transfer entropy is more-or-less identical to Lizier's local transfer entropy. Also note they they do nothing to check if they're getting the 'right' answer, since they do not perform any theoretical work to compute the population level transfer entropies, etc.</li>
</ul>
</li>
<li>
<p>R. Vicente, M. Wibral, M. Lindner, and G. Pipa, "Transfer entropy-a model-free measure of effective connectivity for the neurosciences," Journal of computational neuroscience, 2011.</p>
<ul>
<li>The main / original 'transfer entropy for neuroscience' paper. This uses the estimator from the paper above (which was an arXiv preprint long before it was an <em>Entropy</em> article) that is KSG-like. It also goes into a great deal of detail about specific problems that arise with data resulting from electrophysiology (causal delays, volume conduction, etc.). It does not, however, give any <em>proofs</em> that its proposed estimator works for transfer entropy, nor does it attempt to get the 'right' answer for the transfer entropy. Instead, it focuses on "is there / isn't there?'' with causal connections via surrogates.</li>
</ul>
</li>
<li>
<p>J. Zhu, J. J. Bellanger, H. Shu, and R. Le Bouquin Jeannes, "Contribution to Transfer Entropy Estimation via the k-Nearest-Neighbors Approach," Entropy, 2015.</p>
<ul>
<li>This paper outlines all of the various forms of common transfer entropy estimators, including the ones proposed in the two above papers. Morever, this paper <em>explicitly</em> compares the estimated transfer entropies to a 'true' (model) values, and addresses the fact that transfer entropy is being computed from a correlated . There is none of the song-and-dance around "is there / isn't there?" significance testing for non-zero transfer entropies, and instead the authors focus on the <em>error</em> associated with the transfer entorpy estimator. They also note that both the estimator they propose, and the already existing estimators, do a poor job of approximating the true transfer entropy for a nonlinear system (unpublished work).</li>
</ul>
</li>
</ul>
<p>
  posted at: 14:50 |
  path: <a href="http://ddarmon.github.io/master/notebooks/" title="path">/</a> |
  <a href="http://ddarmon.github.io/master/notebooks/estimators-te-continuous.html">permanent link to this entry</a>
</p>
</div>

<p>
  <a href="http://pyblosxom.github.com/"><img src="http://pyblosxom.github.com/images/pb_pyblosxom.gif" alt="Made with Pyblosxom" border="0" /></a>
</p>
</body>
</html>
