<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Practically Insignificant  | Salvaging Lost Significance via Randomization: Randomized \(P\)-values for Discrete Test Statistics</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.62.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Salvaging Lost Significance via Randomization: Randomized \(P\)-values for Discrete Test Statistics" />
<meta property="og:description" content="Last time, we saw that when performing a hypothesis test with a discrete test statistic, we will typically lose size unless we happen to be very lucky and have the significance level \(\alpha\) exactly match one of our possible \(P\)-values. In this post, I will introduce a randomized hypothesis test that will regain the size we lost. Unlike a lot of randomization in statistics, the randomization here comes at the end: we randomize the \(P\)-value in order to recover the size." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/salvaging-lost-significance-via-randomization-randomized-p-values-for-discrete-test-statistics/" />
<meta property="article:published_time" content="2020-06-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-06-28T00:00:00+00:00" />
<meta itemprop="name" content="Salvaging Lost Significance via Randomization: Randomized \(P\)-values for Discrete Test Statistics">
<meta itemprop="description" content="Last time, we saw that when performing a hypothesis test with a discrete test statistic, we will typically lose size unless we happen to be very lucky and have the significance level \(\alpha\) exactly match one of our possible \(P\)-values. In this post, I will introduce a randomized hypothesis test that will regain the size we lost. Unlike a lot of randomization in statistics, the randomization here comes at the end: we randomize the \(P\)-value in order to recover the size.">
<meta itemprop="datePublished" content="2020-06-28T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-06-28T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="958">



<meta itemprop="keywords" content="statistics," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Salvaging Lost Significance via Randomization: Randomized \(P\)-values for Discrete Test Statistics"/>
<meta name="twitter:description" content="Last time, we saw that when performing a hypothesis test with a discrete test statistic, we will typically lose size unless we happen to be very lucky and have the significance level \(\alpha\) exactly match one of our possible \(P\)-values. In this post, I will introduce a randomized hypothesis test that will regain the size we lost. Unlike a lot of randomization in statistics, the randomization here comes at the end: we randomize the \(P\)-value in order to recover the size."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      Practically Insignificant
    </a>
    <div class="flex-l items-center">
      

      
      












    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">Salvaging Lost Significance via Randomization: Randomized \(P\)-values for Discrete Test Statistics</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-06-28T00:00:00Z">June 28, 2020</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">


<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<!-- 

280620, 57 min

-->
<p>Last time, we saw that when performing a hypothesis test with a discrete test statistic, we will typically lose size unless we happen to be very lucky and have the significance level <span class="math inline">\(\alpha\)</span> exactly match one of our possible <span class="math inline">\(P\)</span>-values. In this post, I will introduce a randomized hypothesis test that will regain the size we lost. Unlike a lot of randomization in statistics, the randomization here comes at the end: we randomize the <span class="math inline">\(P\)</span>-value in order to recover the size. In many cases (ideally!), experimentalists use randomization at the beginning of their experiment, by (ideally) randomly sampling from a population and then randomly assigning units to a treatment. But the extra step of randomizing at the end, when they’re eagerly awaiting to find out whether they’ve been so-blessed with significance stars, they may balk at randomization, even though it makes their test more powerful. I will allow them their balking (I might, too, if my career depended on accumulating many, many stickers), and in the next post I will discuss a method that gets rid of the random element of randomized <span class="math inline">\(P\)</span>-values while still guarding against the conservativeness of the test.</p>
<p>For this exercise, let’s consider a right-sided test. (Why? Because I had already made all the figures and schematics for a right-sided test before writing the last post, and don’t want to remake them for the left-sided test!) Again, consider the case of performing a hypothesis test
<span class="math display">\[\begin{aligned} H_{0} &amp;: p \leq p_{0} \\ H_{1} &amp;: p &gt; p_{0}\end{aligned}\]</span>
for a binomial proportion <span class="math inline">\(p\)</span> using a binomial random variable <span class="math inline">\(X \sim \mathrm{Binom}(n, p)\)</span>. The classical right-sided <span class="math inline">\(P\)</span>-value is then
<span class="math display">\[P = P_{p_{0}}(X \geq x_{\mathrm{obs}}).\]</span>
We know that this will, generally, be slightly too small at a boundary value of <span class="math inline">\(x_{\mathrm{obs}}\)</span>: the difference between the largest <span class="math inline">\(P\)</span>-value less than or equal to <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\alpha\)</span> will be non-zero unless we happen to have a “nice” sample size where that <span class="math inline">\(P\)</span>-value is close to <span class="math inline">\(\alpha\)</span>. But we also know that <span class="math inline">\(P_{p_{0}}(X &gt; x_{\mathrm{obs}})\)</span> will be slightly too big, since this is the first <span class="math inline">\(P\)</span>-value that is stricly greater than <span class="math inline">\(\alpha\)</span>. The idea of a randomized <span class="math inline">\(P\)</span>-value is to add a little fuzz to the classical right-sided <span class="math inline">\(P\)</span>-value that will place us somewhere between <span class="math inline">\(P_{p_{0}}(X \geq x_{\mathrm{obs}})\)</span> and <span class="math inline">\(P_{p_{0}}(X &gt; x_{\mathrm{obs}})\)</span>. That is, the randomized <span class="math inline">\(P\)</span>-value is
<span class="math display">\[P_{\mathrm{rand}} = P_{p_{0}}(X &gt; x_{\mathrm{obs}}) + U \times P_{p_{0}}(X = x_{\mathrm{obs}})\]</span>
where <span class="math inline">\(U\)</span> is a uniform random variable on <span class="math inline">\((0, 1)\)</span> that is independent of <span class="math inline">\(X\)</span>.</p>
<p>Why does this work? Consider the sketch below, which shows the survival function <span class="math inline">\(S(k) = P(X &gt; k)\)</span> and its left-continuous analog <span class="math inline">\(S(k^{-}) = P(X \geq k)\)</span>:</p>
<pre class="r"><code>n &lt;- 10
p &lt;- 0.5

alpha = 0.4

ns &lt;- 0:n

Ss &lt;- pbinom(ns, n, p, lower.tail = FALSE)

par(mar=c(5,5,2,1), cex.lab = 2, cex.axis = 2)
plot(ns, Ss, type = &#39;s&#39;,
     xlab = expression(italic(k)), ylab = &#39;Probability&#39;)

Ss.p1 &lt;- Ss + dbinom(ns, n, p)
points(ns, Ss, col = &#39;blue&#39;, pch = 16)
lines(ns, Ss.p1, col = &#39;red&#39;, type = &#39;p&#39;, pch = 16)

abline(h = alpha, lty = 3, col = &#39;purple&#39;)
abline(h = c(0, 1), lty = 3)
abline(v = c(0, n), lty = 3)

legend(&#39;topright&#39;, legend = c(expression(P(italic(X) &gt;= italic(k))),
                              expression(P(italic(X) &gt; italic(k)))),
       col = c(&#39;red&#39;, &#39;blue&#39;), pch = 1, cex = 1.2)

arrows(0.5, 0, 0.5, alpha, code = 3, col = &#39;purple&#39;, lwd = 2, length = 0.15)
text(x = 0.5, y = alpha+0.05, labels = expression(alpha), col = &#39;purple&#39;, cex = 2)</code></pre>
<p><img src="/post/2020-06-28-salvaging-lost-significance-via-randomization-randomized-p-values-for-discrete-test-statistics_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
Here, <span class="math inline">\(S(k^{-})\)</span> is the classical <span class="math inline">\(P\)</span>-value, <span class="math inline">\(S(k)\)</span> is the “too large” <span class="math inline">\(P\)</span>-value, and the randomized <span class="math inline">\(P\)</span>-value interpolates (randomly) between the classical <span class="math inline">\(P\)</span>-value and the too small <span class="math inline">\(P\)</span>-value:
<span class="math display">\[\begin{aligned} P_{\mathrm{rand}} &amp;= P_{p_{0}}(X &gt; x_{\mathrm{obs}}) + U \times P_{p_{0}}(X = x_{\mathrm{obs}}) \\ &amp;= (1 - U) \times P_{p_{0}}(X &gt; x_{\mathrm{obs}}) + U \times P_{p_{0}}(X \geq x_{\mathrm{obs}}) \\ &amp;= (1-U) S(x_{\mathrm{obs}}) + U S(x_{\mathrm{obs}}^{-}). \end{aligned}\]</span>
The only place where the randomization has any effect occurs where the interpolating line crosses <span class="math inline">\(\alpha\)</span>; otherwise, anywhere we lie on the interpolating line we come to the same decision. Let <span class="math inline">\(k^{*}\)</span> be the first <span class="math inline">\(k\)</span> such that <span class="math inline">\(P(X &gt; k) \leq \alpha\)</span>. For <span class="math inline">\(x_{\mathrm{obs}} &lt; k^{*}\)</span>, we fail to reject <span class="math inline">\(H_{0}\)</span> and for <span class="math inline">\(x_{\mathrm{obs}} &gt; k^{*}\)</span>, we reject <span class="math inline">\(H_{0}\)</span>. When <span class="math inline">\(x_{\mathrm{obs}} = k^{*}\)</span>, we use the randomization device. We should only reject when the randomized <span class="math inline">\(P\)</span>-value is less than or equal to <span class="math inline">\(\alpha\)</span>, and hence when we fall in the portion of the interpolating line that is less than or equal to <span class="math inline">\(\alpha\)</span>. Rotating the figure above 90º counterclockwise, we have
<p>
<img src="https://raw.githubusercontent.com/ddarmon/master/master/images/randomized-p-value-schematic.png" width="600">
<p>
<p>and we should only reject when the <span class="math inline">\(P\)</span>-value falls at or below <span class="math inline">\(\alpha\)</span>. Because <span class="math inline">\(U\)</span> is uniform, the probability that this occurs is
<span class="math display">\[P\left(U \leq \frac{\alpha - S(k^{*})}{p(k^{*})} \right) = \frac{\alpha - S(k^{*})}{p(k^{*})}.\]</span>
So on the boundary of the rejection region, we reject <span class="math inline">\(H_{0}\)</span> with probability <span class="math inline">\(\frac{\alpha - S(k^{*})}{p(k^{*})}\)</span>. Let <span class="math inline">\(\phi(X)\)</span> be our rejection rule, i.e. the function that is 1 when we reject the null hypothesis and 0 otherwise. Then <span class="math inline">\(\phi\)</span> is a random function with the conditional distribution
<span class="math display">\[P(\phi(X) = 1 \mid X = k) = \begin{cases} 0 &amp;: k &lt; k^{*} \\ \frac{\alpha - S(k^{*})}{p(k^{*})} &amp;: k = k^{*} \\ 1 &amp;: k &gt; k^{*} \end{cases}.\]</span>
Given this definition, it’s straightforward to prove that the probability that we reject the null hypothesis, that is, that <span class="math inline">\(\phi(X) = 1\)</span>, is in fact <span class="math inline">\(\alpha\)</span>:
<span class="math display">\[\begin{aligned} P(\phi(X) = 1) &amp;= \sum_{k} P(\phi(X) = 1 \mid X = k) P(X = k) \\ &amp;= \sum_{\{k : k &lt; k^{*}\}} 0 \cdot p(k) + \frac{\alpha - S(k^{*})}{p(k^{*})} p(k^{*}) + \sum_{\{k : k &gt; k^{*}\}} 1 \cdot p(k) \\ &amp;= \alpha - S(k^{*}) + S(k^{*}) \\ &amp;= \alpha,\end{aligned}\]</span>
just as we wanted.</p>
<p>So a little uncertainty at the end buys us back our size, and we attain the desired significance level.</p>
<ul class="pa0">
  
   <li class="list">
     <a href="/tags/statistics" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">statistics</a>
   </li>
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/the-land-of-lost-significance-exact-tests-with-discrete-test-statistics/">Quantum Statistics: Exact Tests with Discrete Test Statistics</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="/" >
    &copy; 2020 Practically Insignificant
  </a>
    <div>











</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
