}
# Remove example with NAs
if (sum(is.na(bootstrap.df)) > 0){
which.na = which(is.na(bootstrap.df), arr.ind = TRUE)
bootstrap.df = bootstrap.df[-which.na[1], ]
my.df = my.df[-which.na[1], ]
}
# 1 - ter_true
# 2 - ter_estimate_biasadjusted
# 3-5 - ci_pivotal[0], ci_pivotal[1], is_in_ci_pivotal*1
# 6-8 - ci_normal[0], ci_normal[1], is_in_ci_normal*1
# 9-11 - ci_percentile[0], ci_percentile[1], is_in_ci_percentile*1
# 12 - B
# 13 - bootstrap_sd
# 14 - ter.estimate.verylong
# 15 - bias.adjustment
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Note: We need to do the following, since the
# globs that combined the ci_results files and the
# ters files did not return the files in the same
# order.
#
# 201218-15-01
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
perc.0 = my.df[, 9]
perc.1 = apply(bootstrap.df, 1, quantile, probs = c(0.025))
# cbind(perc.0, perc.1)
# Reorder based on the 2.5th percentile:
# resort.bootstrap.df = rep(0, nrow(bootstrap.df))
#
# for (s.ind in 1:length(perc.1)){
#   s = perc.1[s.ind]
#   which.out = which(abs(perc.0 - s) < 1e-10)
#
#   if(length(which.out) > 1){
#     cat("Warning: This percentile matches to two different entries in my.df!\n")
#   }else if (length(which.out) == 0){
#     cat("Warning: Found no matches.\n")
#   }
#
#
#   resort.bootstrap.df[s.ind] = which.out
# }
#
# my.df = my.df[resort.bootstrap.df, ]
# Make sure the standard deviations and percentiles
# match after the resorting:
sd.0 = my.df[, 13]
sd.1 = apply(bootstrap.df, 1, sd)
to.plot = 1:1000
par(mfrow = c(1, 1))
plot(sd.0[to.plot], sd.1[to.plot])
perc.0 = my.df[, 9]
perc.1 = apply(bootstrap.df, 1, quantile, probs = c(0.025))
# plot(perc.0[to.plot], perc.1[to.plot])
if (model.name == 'slorenz'){
if (measure.type == 'ter'){
ter.true = 1.21 # for slorenz
}else if (measure.type == 'ais'){
ter.true = 2.2175
}
}else{
ter.true = my.df[1, 1]
}
bias.adjustment = my.df[, 15]
xlim.use = ter.true + 0.5*c(-1, 1)
# I seem to be getting something like double the coverage of
# what I should be? What error am I making here?
# The error (in thinking): I used the variability of
# the *shorter* time series to construct the width
# of the confidence intervals. But these confidence
# intervals were *centered* at the verylong
# estimate, which will have a **less variability**,
# thus making the confidence intervals longer than
# they have to be.
# So you really do want to center the intervals at
# the original debiased estimate for this to work.
# DMD, 201218
# c = 0.95
c = 0.95
alpha = 1 - c
# Center at the verylong estimate:
bs.perc = apply(X = bootstrap.df, MARGIN = 1, FUN = quantile, probs = c(alpha/2, 1 - alpha/2))
bs.perc = t(bs.perc)
if (center.at == 'debiased'){
# Center at the bias-adjusted estimate:
bs.perc = bs.perc - (my.df[, 14] - my.df[, 2])
}
S = nrow(my.df)
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Original normal confidence interval, using the
# bias-adjusted point estimate as the center:
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
is.in.normal.ci = (my.df[, 6] <= ter.true) & (my.df[, 7] >= ter.true)
# par(mar=c(5,5,2,1), cex.lab = 2, cex.axis = 2, mfrow = c(2, 2), pty = 'm')
# plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = 'Old Normal-based')
# segments(x0 = my.df[, 6], x1 = my.df[, 7], y0 = 1:S, col = is.in.normal.ci + 2)
# points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
# abline(v = ter.true, col = 'blue')
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# T-based interval, using the verylong estimate
# as the center.
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
E = qt(1-alpha/2, df = my.df[1, 12] - 1)*my.df[, 13]
if (center.at == 'verylong'){
# Center at verylong estimate:
ci.t.verylong = cbind(my.df[, 14] - E, my.df[, 14] + E)
}else if (center.at == 'debiased'){
# Center at debiased original estimate:
ci.t.verylong = cbind(my.df[, 2] - E, my.df[, 2] + E)
}
is.in.t.ci = (ci.t.verylong[, 1] <= ter.true) & (ci.t.verylong[, 2] >= ter.true)
par(mar=c(5,5,2,1), cex.lab = 2, cex.axis = 2, mfrow = c(1, 2), pty = 'm')
plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = sprintf('T-based, chat = %.3f', mean(is.in.t.ci)))
segments(x0 = ci.t.verylong[, 1], x1 = ci.t.verylong[, 2], y0 = 1:S, col = is.in.t.ci + 2)
if (center.at == 'verylong'){
points(my.df[, 14], 1:S, pch = 16, cex = 0.5)
} else if (center.at == 'debiased'){
points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
}
abline(v = ter.true, col = 'blue')
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Basic bootstrap 2*thetahat - quantiles
# using thetahat = verylong estimate
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
ci.basic.verylong = cbind(2*my.df[, 14] - bs.perc[, 2], 2*my.df[, 14] - bs.perc[, 1])
is.in.basic.ci = (ci.basic.verylong[, 1] <= ter.true) & (ci.basic.verylong[, 2] >= ter.true)
#
# # par(mfrow = c(1, 1), pty = 'm')
# plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = sprintf('Basic, chat = %.3f', mean(is.in.basic.ci)))
# segments(x0 = ci.basic.verylong[, 1], x1 = ci.basic.verylong[, 2], y0 = 1:S, col = is.in.basic.ci + 2)
# # points(my.df[, 14], 1:S, pch = 16, cex = 0.5)
# points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
# abline(v = ter.true, col = 'blue')
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Original percentile intervals, where the
# percentiles are already bias-adjusted, since the
# mean of the bootstraps is equal to the
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
is.in.percentile.ci = (bs.perc[, 1] <= ter.true) & (bs.perc[, 2] >= ter.true)
plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = sprintf('Percentile, chat = %.3f', mean(is.in.percentile.ci)))
segments(x0 = bs.perc[, 1], x1 = bs.perc[, 2], y0 = 1:S, col = is.in.percentile.ci + 2)
if (center.at == 'verylong'){
points(my.df[, 14], 1:S, pch = 16, cex = 0.5)
} else if (center.at == 'debiased'){
points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
}
abline(v = ter.true, col = 'blue')
bias.hat.long = mean(my.df[, 14] - ter.true)
bias.hat.long
bias.hat.original = mean((my.df[, 14] + my.df[, 15]) - ter.true)
bias.hat.original
bias.hat.debiased = mean(my.df[, 2] - ter.true)
bias.hat.debiased
cat(sprintf('The bias using the original estimator is:\t%f\nThe bias using the debiased estimator is:\t%f\nThe bias using the verylong estimator is:\t%f\n', bias.hat.original, bias.hat.debiased, bias.hat.long))
cat(sprintf('The nominal coverage is c = %f.\n\n', c))
cat(sprintf('The observed coverages of methods are:\n\nT-based\t%f\nBasic\t%f\nPerc.\t%f\nOld Z\t%f\n', mean(is.in.t.ci), mean(is.in.basic.ci), mean(is.in.percentile.ci), mean(is.in.normal.ci)))
#----
# Plot the long and bias/non-bias adjusted estimates of the
# total entropy rate across all B realizations.
# par(mar=c(5,5,2,1), cex.lab = 2, cex.axis = 2, mfrow = c(2, 1), pty = 's')
# plot(my.df[, 2], my.df[, 14], xlim = xlim.use, ylim = xlim.use, xlab = 'TER, Bias Adjusted', ylab = 'TER, Long ESN Sim', pch = 16, cex = 0.5)
# abline(v = ter.true)
# abline(h = ter.true)
# rug(my.df[, 2])
# rug(my.df[, 14], side = 2)
#
# plot(my.df[, 2] + bias.adjustment, my.df[, 14], xlim = xlim.use, ylim = xlim.use, xlab = 'TER, Non-bias Adjusted', ylab = 'TER, Long ESN Sim', pch = 16, cex = 0.5)
# abline(v = ter.true)
# abline(h = ter.true)
# rug(my.df[, 2] + bias.adjustment)
# rug(my.df[, 14], side = 2)
#----
cs = seq(0.01, 0.99, by = 0.01)
alphas = 1 - cs
tval = qt(1-alphas/2, df = my.df[1, 12] - 1)
s = my.df[, 13]
# Want E to be (# sims) X (# coverage levels)
Es = matrix(0, nrow = length(s), ncol = length(cs))
for (sim.ind in 1:length(s)){
Es[sim.ind, ] = s[sim.ind]*tval
}
if (center.at == 'debiased'){
ci.left  = my.df[, 2] - Es
ci.right = my.df[, 2] + Es
}else if (center.at == 'verylong'){
ci.left  = my.df[, 14] - Es
ci.right = my.df[, 14] + Es
}
emp.cs = rep(0, length(cs))
for (c.ind in 1:length(cs)){
emp.cs[c.ind] = mean((ter.true >= ci.left[, c.ind]) & ter.true <= ci.right[, c.ind])
}
par(mfrow = c(1, 1), pty = 's')
plot(cs, emp.cs, type = 'l', xlim = c(0, 1), ylim = c(0, 1), xlab = 'Nominal Coverage', ylab = 'Empirical Coverage', lwd = 2)
abline(a = 0, b = 1, lty = 2)
emp.nums = emp.cs*nrow(bootstrap.df)
# install.packages('binom')
library(binom)
c = 0.95
alpha = 1 - c
confints = binom.confint(x = emp.nums, conf.level = 1 - alpha, n = nrow(bootstrap.df), methods = 'ac')
par(pty = 's')
plot(cs, confints$lower, type = 'l', xlim = c(0, 1), ylim = c(0,1), col = 'red', xlab = 'Nominal Coverage', ylab = 'Empirical Coverage')
lines(cs, confints$upper, type = 'l', col = 'red')
lines(cs, emp.cs, col = 'blue')
abline(a = 0, b = 1, lty = 2)
c = 0.95
alpha = 1 - c
m = length(cs) # Make simultaneous band at each coverage level
confints = binom.confint(x = emp.nums, conf.level = 1 - alpha/m, n = nrow(bootstrap.df), methods = 'ac')
par(pty = 's')
plot(cs, confints$lower, type = 'l', xlim = c(0, 1), ylim = c(0,1), col = 'red', xlab = 'Nominal Coverage', ylab = 'Empirical Coverage', lwd = 2)
lines(cs, confints$upper, type = 'l', col = 'red', lwd = 2)
lines(cs, emp.cs, col = 'blue', lwd = 2)
abline(a = 0, b = 1, lty = 2)
setwd('/Users/daviddarmon/Dropbox (Personal)/Reference/T/tirp/2018/bootstrap-infodynamics/experiment-with-alternative-cis')
center.at = 'verylong'
# center.at = 'debiased'
measure.type = 'ter'
# measure.type = 'ais'
if (measure.type == 'ais'){
measure.type.suffix = 'ais.'
} else{
measure.type.suffix = ''
}
# N = 1000
N = 10000
# model.name = 'slogistic'
# model.name = 'slorenz'
model.name = 'setar'
# prefix = 'old-results/1_possible-misalign/'
# prefix = 'old-results/2_same-as-above/'
prefix = 'old-results/12_from_monmouth_B2000_eigen_rescale/'
# prefix = ''
fname.prefix = sprintf('%s%s-N%g_UMD_wBiasAdjustment', prefix, model.name, N)
my.df = read.csv(sprintf('%s.%sci_results', fname.prefix, measure.type.suffix), header = FALSE)
if(file.exists(sprintf('%s.%ss.RData', fname.prefix, measure.type))){
load(file = sprintf('%s.%ss.RData', fname.prefix, measure.type))
}else{
bootstrap.df = read.csv(sprintf('%s.%ss', fname.prefix, measure.type), header = FALSE)
bootstrap.df = data.matrix(bootstrap.df)
save(bootstrap.df, file = sprintf('%s.%ss.RData', fname.prefix, measure.type))
}
# Remove example with NAs
if (sum(is.na(bootstrap.df)) > 0){
which.na = which(is.na(bootstrap.df), arr.ind = TRUE)
bootstrap.df = bootstrap.df[-which.na[1], ]
my.df = my.df[-which.na[1], ]
}
# 1 - ter_true
# 2 - ter_estimate_biasadjusted
# 3-5 - ci_pivotal[0], ci_pivotal[1], is_in_ci_pivotal*1
# 6-8 - ci_normal[0], ci_normal[1], is_in_ci_normal*1
# 9-11 - ci_percentile[0], ci_percentile[1], is_in_ci_percentile*1
# 12 - B
# 13 - bootstrap_sd
# 14 - ter.estimate.verylong
# 15 - bias.adjustment
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Note: We need to do the following, since the
# globs that combined the ci_results files and the
# ters files did not return the files in the same
# order.
#
# 201218-15-01
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
perc.0 = my.df[, 9]
perc.1 = apply(bootstrap.df, 1, quantile, probs = c(0.025))
# cbind(perc.0, perc.1)
# Reorder based on the 2.5th percentile:
# resort.bootstrap.df = rep(0, nrow(bootstrap.df))
#
# for (s.ind in 1:length(perc.1)){
#   s = perc.1[s.ind]
#   which.out = which(abs(perc.0 - s) < 1e-10)
#
#   if(length(which.out) > 1){
#     cat("Warning: This percentile matches to two different entries in my.df!\n")
#   }else if (length(which.out) == 0){
#     cat("Warning: Found no matches.\n")
#   }
#
#
#   resort.bootstrap.df[s.ind] = which.out
# }
#
# my.df = my.df[resort.bootstrap.df, ]
# Make sure the standard deviations and percentiles
# match after the resorting:
sd.0 = my.df[, 13]
sd.1 = apply(bootstrap.df, 1, sd)
to.plot = 1:1000
par(mfrow = c(1, 1))
plot(sd.0[to.plot], sd.1[to.plot])
perc.0 = my.df[, 9]
perc.1 = apply(bootstrap.df, 1, quantile, probs = c(0.025))
# plot(perc.0[to.plot], perc.1[to.plot])
if (model.name == 'slorenz'){
if (measure.type == 'ter'){
ter.true = 1.21 # for slorenz
}else if (measure.type == 'ais'){
ter.true = 2.2175
}
}else{
ter.true = my.df[1, 1]
}
bias.adjustment = my.df[, 15]
xlim.use = ter.true + 0.5*c(-1, 1)
# I seem to be getting something like double the coverage of
# what I should be? What error am I making here?
# The error (in thinking): I used the variability of
# the *shorter* time series to construct the width
# of the confidence intervals. But these confidence
# intervals were *centered* at the verylong
# estimate, which will have a **less variability**,
# thus making the confidence intervals longer than
# they have to be.
# So you really do want to center the intervals at
# the original debiased estimate for this to work.
# DMD, 201218
# c = 0.95
c = 0.95
alpha = 1 - c
# Center at the verylong estimate:
bs.perc = apply(X = bootstrap.df, MARGIN = 1, FUN = quantile, probs = c(alpha/2, 1 - alpha/2))
bs.perc = t(bs.perc)
if (center.at == 'debiased'){
# Center at the bias-adjusted estimate:
bs.perc = bs.perc - (my.df[, 14] - my.df[, 2])
}
S = nrow(my.df)
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Original normal confidence interval, using the
# bias-adjusted point estimate as the center:
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
is.in.normal.ci = (my.df[, 6] <= ter.true) & (my.df[, 7] >= ter.true)
# par(mar=c(5,5,2,1), cex.lab = 2, cex.axis = 2, mfrow = c(2, 2), pty = 'm')
# plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = 'Old Normal-based')
# segments(x0 = my.df[, 6], x1 = my.df[, 7], y0 = 1:S, col = is.in.normal.ci + 2)
# points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
# abline(v = ter.true, col = 'blue')
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# T-based interval, using the verylong estimate
# as the center.
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
E = qt(1-alpha/2, df = my.df[1, 12] - 1)*my.df[, 13]
if (center.at == 'verylong'){
# Center at verylong estimate:
ci.t.verylong = cbind(my.df[, 14] - E, my.df[, 14] + E)
}else if (center.at == 'debiased'){
# Center at debiased original estimate:
ci.t.verylong = cbind(my.df[, 2] - E, my.df[, 2] + E)
}
is.in.t.ci = (ci.t.verylong[, 1] <= ter.true) & (ci.t.verylong[, 2] >= ter.true)
par(mar=c(5,5,2,1), cex.lab = 2, cex.axis = 2, mfrow = c(1, 2), pty = 'm')
plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = sprintf('T-based, chat = %.3f', mean(is.in.t.ci)))
segments(x0 = ci.t.verylong[, 1], x1 = ci.t.verylong[, 2], y0 = 1:S, col = is.in.t.ci + 2)
if (center.at == 'verylong'){
points(my.df[, 14], 1:S, pch = 16, cex = 0.5)
} else if (center.at == 'debiased'){
points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
}
abline(v = ter.true, col = 'blue')
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Basic bootstrap 2*thetahat - quantiles
# using thetahat = verylong estimate
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
ci.basic.verylong = cbind(2*my.df[, 14] - bs.perc[, 2], 2*my.df[, 14] - bs.perc[, 1])
is.in.basic.ci = (ci.basic.verylong[, 1] <= ter.true) & (ci.basic.verylong[, 2] >= ter.true)
#
# # par(mfrow = c(1, 1), pty = 'm')
# plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = sprintf('Basic, chat = %.3f', mean(is.in.basic.ci)))
# segments(x0 = ci.basic.verylong[, 1], x1 = ci.basic.verylong[, 2], y0 = 1:S, col = is.in.basic.ci + 2)
# # points(my.df[, 14], 1:S, pch = 16, cex = 0.5)
# points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
# abline(v = ter.true, col = 'blue')
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# Original percentile intervals, where the
# percentiles are already bias-adjusted, since the
# mean of the bootstraps is equal to the
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
is.in.percentile.ci = (bs.perc[, 1] <= ter.true) & (bs.perc[, 2] >= ter.true)
plot(0, 0, cex = 0, xlim = xlim.use, ylim = c(0, S), main = sprintf('Percentile, chat = %.3f', mean(is.in.percentile.ci)))
segments(x0 = bs.perc[, 1], x1 = bs.perc[, 2], y0 = 1:S, col = is.in.percentile.ci + 2)
if (center.at == 'verylong'){
points(my.df[, 14], 1:S, pch = 16, cex = 0.5)
} else if (center.at == 'debiased'){
points(my.df[, 2], 1:S, pch = 16, cex = 0.5)
}
abline(v = ter.true, col = 'blue')
bias.hat.long = mean(my.df[, 14] - ter.true)
bias.hat.long
bias.hat.original = mean((my.df[, 14] + my.df[, 15]) - ter.true)
bias.hat.original
bias.hat.debiased = mean(my.df[, 2] - ter.true)
bias.hat.debiased
cat(sprintf('The bias using the original estimator is:\t%f\nThe bias using the debiased estimator is:\t%f\nThe bias using the verylong estimator is:\t%f\n', bias.hat.original, bias.hat.debiased, bias.hat.long))
cat(sprintf('The nominal coverage is c = %f.\n\n', c))
cat(sprintf('The observed coverages of methods are:\n\nT-based\t%f\nBasic\t%f\nPerc.\t%f\nOld Z\t%f\n', mean(is.in.t.ci), mean(is.in.basic.ci), mean(is.in.percentile.ci), mean(is.in.normal.ci)))
#----
# Plot the long and bias/non-bias adjusted estimates of the
# total entropy rate across all B realizations.
# par(mar=c(5,5,2,1), cex.lab = 2, cex.axis = 2, mfrow = c(2, 1), pty = 's')
# plot(my.df[, 2], my.df[, 14], xlim = xlim.use, ylim = xlim.use, xlab = 'TER, Bias Adjusted', ylab = 'TER, Long ESN Sim', pch = 16, cex = 0.5)
# abline(v = ter.true)
# abline(h = ter.true)
# rug(my.df[, 2])
# rug(my.df[, 14], side = 2)
#
# plot(my.df[, 2] + bias.adjustment, my.df[, 14], xlim = xlim.use, ylim = xlim.use, xlab = 'TER, Non-bias Adjusted', ylab = 'TER, Long ESN Sim', pch = 16, cex = 0.5)
# abline(v = ter.true)
# abline(h = ter.true)
# rug(my.df[, 2] + bias.adjustment)
# rug(my.df[, 14], side = 2)
#----
cs = seq(0.01, 0.99, by = 0.01)
alphas = 1 - cs
tval = qt(1-alphas/2, df = my.df[1, 12] - 1)
s = my.df[, 13]
# Want E to be (# sims) X (# coverage levels)
Es = matrix(0, nrow = length(s), ncol = length(cs))
for (sim.ind in 1:length(s)){
Es[sim.ind, ] = s[sim.ind]*tval
}
if (center.at == 'debiased'){
ci.left  = my.df[, 2] - Es
ci.right = my.df[, 2] + Es
}else if (center.at == 'verylong'){
ci.left  = my.df[, 14] - Es
ci.right = my.df[, 14] + Es
}
emp.cs = rep(0, length(cs))
for (c.ind in 1:length(cs)){
emp.cs[c.ind] = mean((ter.true >= ci.left[, c.ind]) & ter.true <= ci.right[, c.ind])
}
par(mfrow = c(1, 1), pty = 's')
plot(cs, emp.cs, type = 'l', xlim = c(0, 1), ylim = c(0, 1), xlab = 'Nominal Coverage', ylab = 'Empirical Coverage', lwd = 2)
abline(a = 0, b = 1, lty = 2)
emp.nums = emp.cs*nrow(bootstrap.df)
# install.packages('binom')
library(binom)
c = 0.95
alpha = 1 - c
confints = binom.confint(x = emp.nums, conf.level = 1 - alpha, n = nrow(bootstrap.df), methods = 'ac')
par(pty = 's')
plot(cs, confints$lower, type = 'l', xlim = c(0, 1), ylim = c(0,1), col = 'red', xlab = 'Nominal Coverage', ylab = 'Empirical Coverage')
lines(cs, confints$upper, type = 'l', col = 'red')
lines(cs, emp.cs, col = 'blue')
abline(a = 0, b = 1, lty = 2)
c = 0.95
alpha = 1 - c
m = length(cs) # Make simultaneous band at each coverage level
confints = binom.confint(x = emp.nums, conf.level = 1 - alpha/m, n = nrow(bootstrap.df), methods = 'ac')
par(pty = 's')
plot(cs, confints$lower, type = 'l', xlim = c(0, 1), ylim = c(0,1), col = 'red', xlab = 'Nominal Coverage', ylab = 'Empirical Coverage', lwd = 2)
lines(cs, confints$upper, type = 'l', col = 'red', lwd = 2)
lines(cs, emp.cs, col = 'blue', lwd = 2)
abline(a = 0, b = 1, lty = 2)
?signif
1-pnorm(0.171)
1-pnorm(0.25)
?qnorm
qt(0.975, 99)
library(extrafont)
loadfonts()
fonts
fonts()
extrafont::fonts()
extrafont::fonttable()
font_install('fontcm')
extrafont::fonttable()
1-pnorm(1.375)
(1-pnorm(1.375))/(1-pnorm(0.875))
pnorm(1.173) - pnorm(-1.29)
source('~/Documents/R/stack-exchange-model.R')
library(rgl)
par3d?
?par3d
?rotationMatrix
?sprintf
?cat
write.table
?write.table
setwd('/Users/daviddarmon/Dropbox (Personal)/Reference/T/teaching/201819/2019sp/ma220/lesson-plans/2/r-lab')
setwd('/Users/daviddarmon/Documents/Reference/G/github/master/teaching/MA-220-SP19/lesson-plans/2')
knitr::opts_chunk$set(echo = TRUE)
weightloss.data = read.csv('weight-change-lowcarb.txt')
View(weightloss.data)
View(weightloss.data)
weightloss.data = read.csv('weight-change-lowfat.txt')
View(weightloss.data)
