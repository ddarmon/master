<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<h1>Homework 8</h1>

<p><h1>Chapter 7</h1>
<p> Complete the following 4 problems:
<ol>
	<li>Problem 7.3b
		<ul>
			<li>Test the null hypothesis using a \(T\)-statistic (<b>not</b> the \(F\)-statistic) under MLRGN model assumptions <b>and</b> using <tt>confcurve</tt>'s <tt>confpvalue</tt>.
			<li>Perform the bootstrapping with <tt>B = 10000</tt> bootstrap replicates.
			<li>Comment on the difference between the inferences, if any.
		</ul>
	<li>Problem 7.6
		<ul>
			<li>Test the null hypothesis by checking the inclusion / exclusion of the null parameter values in an appropriate confidence ellipse.
			<li>Approximate the \(P\)-value of the sample estimates under the given null hypothesis by constructing the appropriate confidence ellipse. You should approximate the \(P\)-value to <b>at least two significant digits</b>.
		</ul>
	<li>Problem 7.8
		<ul>
			<li>Follow the same directions as Problem 7.6.
		</ul>
	<li>Problem 7.9
		<ul>
			<li>Follow the same directions as Problem 7.6.
		</ul>
</ol>

<p><b>Hint for Problems 7.6, 7.8, and 7.9:</b> The function <tt>confidenceEllipse</tt> in the <tt>car</tt> package will return a matrix with the \(x\) and \(y\) coordinates of the confidence ellipse that you can plot in Base R using:
<p>
<code>plot(confidenceEllipse(...), type = 'l')</code>
<p>
<p>where the <tt>...</tt> should be the appropriate arguments to <tt>confidenceEllipse</tt>.
<p>You can also add a single \((x, y)\) point to a plot in Base R by using the command
<p>
<code>points(x, y)</code>

<p><h1>Additional Problems</h1>
<p> Read Section 9.2.1 of <a href="http://www.stat.cmu.edu/~cshalizi/TALR/">The Truth About Linear Regression</a><a href="#fn:1" id="fnref:1" title="see footnote" class="footnote">[1]</a> about interpreting coefficients in a linear regression after log-transforming the response. Then answer the following questions.

<p><b>Note:</b> Everywhere you see a \(\log\), you should assume it is the <b>natural logarithm</b>. As some of you have already heard me say, there is only one logarithm, and it is the natural logarithm.

<h2>Problem 1</h2>

<p>Assume that the MLRGN model assumptions hold for the multiple linear regression model of the log-response on the predictors:
\[ \log Y_{i} = \beta_{0} + \sum_{j = 1}^{p} \beta_{j} X_{ij} + \epsilon_{i}, i = 1, \ldots, n\]
<ol type="a">
	<li>What does \(\beta_{0}\) tell us about the distribution of the population response? Hint: it is <b>not</b> the expected response when the predictors are zero.
	<li>What does \(\beta_{j}\) for \(j \neq 0\) tell us about the relationship between the population response and the \(j\)th predictor? Hint: it is <b>not</b> the expected increase in the response for each unit increase in the \(j\)th predictor while accounting for the other predictors in the model.
</ol>

<h2>Problem 2</h2>

<p> Consider the simple linear regression model
\[ \log Y = \beta_{0} + \beta_{1} X + \epsilon.\]
As you discovered in the previous problem, \(\beta_{1}\) no longer corresponds to the expected increase in \(Y\) for each unit increase in \(X\). However, we <b>can</b> provide an interpretation for \(\beta_{1}\) that is nearly as easy to understand, as long as \(\beta_{1}\) is sufficiently small in magnitude.

<ol type="a">
	<li>Solve for \(Y\) in the simple linear regression.
	<li>Show that the proportional increase in \(Y\) for each unit increase in \(X\),
		\[ \frac{Y(X + 1) - Y(X)}{Y(X)},\]
		simplifies to \(e^{\beta_{1}} - 1\).
	<li>Derive (or find) the <a href="https://en.wikipedia.org/wiki/Taylor_series#List_of_Maclaurin_series_of_some_common_functions">Maclaurin series expansion</a> of \(e^{\beta_{1}}\).
	<li>Explain why, for small enough \(\beta_{1}\), we can approximate \( e^{\beta_{1}} - 1\) by \(\beta_{1}\).
	<li>Provide an interpretation for \(\beta_{1}\) given this fact, and demonstrate that this is the case for \(\beta_{1} = 0.05\) and \(\beta_{1} = -0.05\).
</ol>

<div class="footnotes">
<hr />
<ol>

<li id="fn:1">
<p>This is one of my favorite textbooks on regression, by one of my favorite statisticians, <a href="http://www.stat.cmu.edu/~cshalizi/">Cosma Shalizi</a>. As the title suggests, it cuts past a lot of b******t you might read / hear about linear regression and provides just the facts about what a regression can (and cannot) say about a statistical question. If you ever want to know more about linear regression, I strongly recommend this book. <a href="#fnref:1" title="return to article" class="reversefootnote">&#160;&#8617;</a></p>
</li>

</ol>
</div>
