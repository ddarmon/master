<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<title>MA 440-01 Regression and Time Series Analysis (Fall 2019)</title>

<a href="http://www.thirdorderscientist.org"><em>David Darmon</em></a>

<h1>MA 440-01, Regression and Time Series Analysis</h1>

<h2>Fall 2019</h2>


<center>
Section 01: Tuesday and Friday, 11:40 AM &mdash; 1:00 PM, Howard Hall 308
</center>

<P>Here's the official description:
<blockquote>Covers topics related to multiple regression techniques, including testing the assumptions required for each to be valid. This includes applications to yield curve smoothing, pricing, and investment models, and the use of principal component analysis. Also covered are techniques for the analysis and modeling of time series data and forecasting.</blockquote>

<P>This course covers the process of statistical analysis from beginning to end. That process, in broad strokes, is as follows: we pose a scientific question, determine what experiments or observations might provide data towards answering that question, develop approaches to collecting that data, summarize the resulting data, and derive inferences relevant to the original scientific question. In the process, we will learn about sampling, descriptive statistics, probability, probability models, inferential statistics, confidence intervals, hypothesis tests, and regression. A special emphasis will be given to common pitfalls in statistical analysis you are likely to see 'in the wild', including mis-interpretations / mis-understandings of statistical procedures, the conflation of association and causation, and the reproducibility crisis in psychology, medicine, and nutrition. 

<h4>Prerequisites</h4>

<P>MA 116 or MA 118 or MA 126, passed with a grade of C- or higher, and MA 151 or MA 220 or BE 251, passed with a grade of C- or higher.

<h4>Professor</h4>

<table>
<tr><td>Dr. David Darmon</td> <td></td><td>ddarmon [at] monmouth.edu</td></tr>
<tr><td></td><td></td><td>Howard Hall 241</td></tr>
</table>

<h2>Topics, Notes, Readings</h2>

<P>This is currently a <em>tentative</em> listing of topics, in order.


<dl>
	<dt><em>Introduction:</em> What is statistics? Where do data come from? Experimental versus observational studies. Types of data.
    <dt><em>Descriptive statistics as summaries of data:</em> Summaries of the entire data distribution: rug plot, dot plot, histogram, box plot. Measures of center: mean, median, mode. Measures of variation: range, standard deviation, mean absolute deviation.
    <dt><em>Probability:</em> The origin of probability in games of chance. The interpretation of probability. Computing probabilities involving two or more events using the addition and multiplication rules. Conditional probability and independence.
    <dt><em>Discrete random variables:</em> Random variables as 'numbers that could have been otherwise.' Probability mass functions. Parametric models for discrete random variables: uniform, Bernoulli, binomial, geometric, and Poisson. Moments of discrete random variables.
    <dt><em>Continuous random variables:</em> Summarizing quantitative data with histograms. From histograms to probability density functions. Parametric models for continuous random variables: uniform, exponential, normal. Moments of continuous random variables. The normal distribution and its properties. Using the normal distribution to approximate the binomial distribution.
    <dt><em>Statistics and their sampling distributions:</em> From forward probability to inverse probability. Statistics as functions of a data set. The sampling distribution of the sample mean. The Law of Large Numbers and the Central Limit Theorem.
    <dt><em>Estimators and point estimation:</em> Making an educated guess at a population parameter using a point estimator. Desirable properties of point estimators. Methods for deriving point estimators: method of moments and maximum likelihood estimation.
    <dt><em>Confidence intervals:</em> Confidence intervals as interval estimators. The interpretation of confidence intervals. Confidence intervals for population means and proportions.
    <dt><em>Hypothesis tests:</em> Hypothesis tests for population means and proportions. Components of a hypothesis test. Types of error in hypothesis testing. Scientific hypotheses and statistical hypotheses. Statistical significance and practical significance. Using confidence intervals for hypothesis tests. Using confidence intervals to distinguish between practical and statistical significance. <em>P</em>-values.
    <dt><em>Everything but the kitchen sink:</em> Two-sample tests for population means and their associated confidence intervals. Two-sample tests for population proportions and their associated confidence intervals. 
    <dt><em>Correlation and regression:</em> Exploratory data analysis for two or more quantitative variables. Pearson's correlation coefficient. Confidence interval and hypothesis test for the Pearson correlation between two quantitative variables. Association does not imply causation. Simple linear regression. Interpretation of regression coefficients. Multiple linear regression. Predictive and causative statements.
</dl>

See the <a href = "#schedule">end of this page</a> for the current lecture schedule, subject to revision.  Homework and additional resources will be linked there, as available.

<h2>Course Mechanics</h2>

<h4>Office Hours</h4>

<p>I will have office hours at the following four times each week:

<center><table>
    <tr><td>Monday, &nbsp;&nbsp;10:00&mdash;11:00 AM</td> <td>Howard Hall 241</td></tr>
</td></tr>
    <tr><td>Tuesday, &nbsp;&nbsp;03:00&mdash;04:00 PM</td> <td>Howard Hall 241</td></tr>
</td></tr>
    <tr><td>Thursday, 10:00&mdash;11:00 AM</td> <td>Howard Hall 241</td></tr>
</td></tr>
    <tr><td>Thursday, 01:30&mdash;02:30 PM</td> <td>Howard Hall 241</td></tr>
</table></center>

<p>I have an open-door policy during those times: you can show up unannounced. If you cannot make the scheduled office hours, please e-mail me about making an appointment.

<p>If you are struggling with the homework, having difficulty with the quizzes, or just want to chat, please visit me during my office hours. I am here to help.

<h4>Grading Policy</h4> 

Your final grade will be determined by the following weighting scheme:

<dl>
	<dd> 45% for 2 in-class exams (22.5% each)
	<dd> 25% for a cumulative final exam
	<dd> 15% for homework problem sets
	<dd> &nbsp;&nbsp;5% for pre-class preparation
	<dd> &nbsp;&nbsp;5% for quizzes
	<dd> &nbsp;&nbsp;5%  for class participation
</dl>

I will use the standard 10-point breakdown to assign letter grades to numerical grades:
<ul>
	<li> \([90, 100] \to \text{A}\)
	<li> \([80, 90) \,\,\, \to \text{B}\)
	<li> \([70, 80) \,\,\, \to \text{C}\)
	<li> \([60, 70) \,\,\, \to \text{D}\)
	<li> \([0, 60) \,\,\,\,\,\, \to \text{F}\)
</ul>

<h4>Homework</h4>

Homework will be assigned at the end of every class meeting, and listed in the <a href="#schedule">Schedule</a> section of this page. Homework assignments are due at the beginning of the next class meeting.

<h4>Pre-class Preparation</h4>

<p>One of the most important skills needed to master mathematics is memory. To do mathematics, you need to make connections between concepts stored in your long-term memory, and before you can do that, you need to store those memories in the first place. One of the best methods for strengthening long-term memory is <strong>retrieval practice</strong> (think flash cards) combined with <strong>spaced repetition</strong> (think reviewing flash cards on an intelligent schedule). This is the exact opposite of how many students study, which typically takes the form of browsing notes (and thus skipping over retrieving the information from their own memories) immediately before the information is needed (i.e. 'cramming'). Unfortunately, this is one of the worst ways to commit information to long-term memory, despite the fact that cramming <em>feels</em> effective in the short-term. Retrieval practice with spaced repetition is more effective than the browse-and-cram approach, takes less time, and is more enjoyable!

<p>As part of pre-class preparation, you are required to regularly use Anki, and submit your Anki decks via eCampus. See <a href = "#anki">below</a> for details on Anki.

<p>See <a href = "MA-220-SP19/additional-syllabus-pages/preclass-prep-instructions.html">here</a> for the instructions on submitting your Anki decks via eCampus.

<h4>Quizzes</h4>

<p> There will be a 10-minute quiz at the beginning of every class. The quizzes will be cumulative, covering material from the first lecture through the most recent lecture. These quizzes are meant to be diagnostic: they direct you to gaps in your understanding of the material so that you can course correct before exams. As long as you are on time for a quiz and make a good-faith attempt to complete the quiz, you will receive full credit for that quiz.

<p> Up to 4 missed quizzes will be dropped. As such, there will be no make-up quizzes.

<h4>Class Participation</h4>

I expect you to be fully engaged during each class, to ask questions when confused, and to attempt to answer questions when called on. If I notice that the same students are always answering each question, I may begin randomly sampling from students. I will announce this before doing so. During class, attempting to answer a question is more important than getting the correct answer, and class participation will be assigned accordingly.

<h4>Attendance</h4>

Required. If you expect to miss 2-3 sessions of the course, you should take the course during another semester.

<h4>Examination Absences</h4>

If you miss an examination your grade will be zero for that exam.  If you know you will be absent for an exam you must let me know at least one week in advance to schedule a make-up exam.

<h4>Textbook</h4>

<P>The <strong>required</strong> textbook is:
<ul>
<li>Jay L. Devore and Kenneth N. Berk. <em>Modern mathematical statistics with applications</em>, 1st Edition (Cengage Learning, 2007). <a href = "http://mubookstore.monmouth.edu/TextBookDetail.aspx?BookPriceID=2292893&MBSNumber=747336&SecID=9614832&trm=SPRING%2019#.XBl34y2ZNhE">Link to University Store</a>
</ul>

<h4>Collaboration, Cheating, and Plagiarism</h4>

All submitted work should be your own. You are welcome and encouraged to consult with others while working on an assignment, including other students in the class and tutors in the <a href="https://www.monmouth.edu/department-mathematics/math-learning-center/">Mathematics Learning Center</a>.  However, whenever you have had assistance with a problem, you must state so at the beginning of the problem solution.  Unless this mechanism is abused, there will be no reduction in credit for using and reporting such assistance.  This policy applies to both individual and group work. In group work, you only need to acknowledge help from outside the group. This policy does <b>not</b> apply to examinations.

<h4>Statement on Special Accommodations</h4>

Students with disabilities who need special accommodations for this class are encouraged to meet with me or the appropriate disability service provider on campus as soon as possible.  In order to receive accommodations, students must be registered with the appropriate disability service provider on campus as set forth in the student handbook and must follow the University procedure for self-disclosure, which is stated in the University <em>Guide to Services and Accommodations for Students with Disabilities</em>.  Students will not be afforded any special accommodations for academic work completed prior to the disclosure of the disability, nor will they be afforded any special accommodations prior to the completion of the documentation process with the appropriate disability office.

<div id="R"><h2>R</h2></div>

<P>We will use R, a programming language for statistical computing, throughout the semester for in-class activities and homework assignments. I will cover the relevant features of R throughout the course.

<p>R will be installed on all of the classroom computers. You should also install R on your personal computer, if you have one. You can install R by following the instructions <a href="https://cran.rstudio.com/bin/windows/base/">for Windows here</a>, <a href="https://cran.rstudio.com/bin/macosx/">for macOS here</a>, or <a href="https://cran.rstudio.com/bin/linux/">for Linux here</a>. You will also want to install RStudio, and Integrated Development Environment for R, which you can find <a href="https://www.rstudio.com/products/rstudio/download/#download">here</a>.

<p>We will use R as a scripting language and statistical calculator, and thus will not get into the nitty-gritty of programming in R. The <a href="https://www.cyclismo.org/tutorial/R/">R Tutorial by Kelly Black</a> is a good reference for the basics of using R. I will demonstrate R's functionality in class and handouts as we need it.

<div id="anki"><h2>Anki</h2></div>

<P> We will use <a href = "https://ankiweb.net/about">Anki</a> for spaced retrieval practice throughout the semester. Anki is open-source, free (as in both <a href = "https://en.wikipedia.org/wiki/Gratis_versus_libre"><em>gratis</em> and <em>libre</em></a>) software. You can download Anki to your personal computer from <a href = "https://apps.ankiweb.net/#download">this link</a>. If you have ever used flashcards, then Anki should be fairly intuitive. If you would like more details you can find Anki's User Manual <a href = "https://apps.ankiweb.net/docs/manual.html">here</a>.

<P> <strong>Note:</strong> Anki has both desktop and mobile phone variants. Please use the desktop variant.

<div id="schedule"><h2>Schedule</h2></div>

<strong>Subject to revision</strong>.  Assignments and solutions will all be linked here, as they are available.  All readings are from the textbook by Devore and Berk unless otherwise noted.

<dl>

<dt>Prior to September 3, Lecture 0:
	<dd><b>Topics: </b>Spaced retrieval practice. Pre-class assessment.
	<dd><a href="MA-440-FA19/lesson-plans/0/homework.html">Pre-class Assignments</a>
		
<dt>September 3, Lecture 1:
	<dd><b>Topics: </b>Introduction to course. Review of Big Ideas in Statistics: random variables, random samples, estimation, and testing. Statistical prediction.
	<dd><b>Sections: </b>

<dt>September 6, Lecture 2:
	<dd><b>Topics: </b>Introduction to R. Exploratory data analysis with R. Histograms, scatter plots, and smoothers.
	<dd><b>Sections: </b>

<dt>September 10, Lecture 3:
	<dd><b>Topics: </b>Statistical modeling. Drawing lines through scatter plots. Which line did it best? Statistical models as data summaries. Statistical models as stories about populations. Sources of uncertainty in inference: sampling variability, measurement error, and chance.
	<dd><b>Sections: </b>

<dt>September 13, Lecture 4:
	<dd><b>Topics: </b>Simple linear regression. The least-squares line, and hints at its derivation. Simple linear regression with a random sample from a population. Stories we tell: the simple linear regression with Gaussian noise (SLRGN) model.
	<dd><b>Sections: </b>

<dt>September 17, Lecture 5:
	<dd><b>Topics: </b>Checking the assumptions of the simple linear model. Residual plots. Out-of-sample generalization. Transformations to handle violations of assumptions.
	<dd><b>Sections: </b>

<dt>September 20, Lecture 6:
	<dd><b>Topics: </b>Properties of the simple linear regression estimators under random sampling from a population. Properties of the simple linear regression estimators under the SLRGN model. Estimation and hypothesis testing under the SLRGN model. The estimate, not the parameter, is significant. What is a P-value, again? Statistical significance and practical significance.
	<dd><b>Sections: </b>

<dt>September 24, Lecture 7:
	<dd><b>Topics: </b>More on estimation and hypothesis testing. The duality between estimation and hypothesis testing. Why 0.05? Use all the numbers: the confidence interval at all confidence levels. Example with the interval estimator for the mean of a Gaussian population. The confidence curve. P-values (if you must) from the confidence curve.
	<dd><b>Sections: </b>

<dt>September 27, Lecture 8:
	<dd><b>Topics: </b>Estimation and hypothesis testing when SLRGN won't do. Bootstrapping. Bootstrap confidence intervals. Bootstrap hypothesis testing. Thank Science for R. Confidence curves from the bootstrap distribution.
	<dd><b>Sections: </b>

<dt>October 1, Lecture 9:
	<dd><b>Topics: </b>Inferences for expected values: standard errors and confidence sets. Inferences for new measurements: standard errors and confidence sets.
	<dd><b>Sections: </b>

<dt>October 4, Lecture 10:
	<dd><b>Topics: </b>But how did we do? Assessing the performance of a simple linear regression model at prediction. In-sample error. Out-of-sample error. Cross-validated error. Bootstrapping estimates of error.
	<dd><b>Sections: </b>

<dt>October 11, Lecture 11:
	<dd><b>Topics: </b>Lab 1.
	<dd><b>Sections: </b>

<dt>October 15, Lecture 12:
	<dd><b>Topics: </b>Exam 1.
	<dd><b>Sections: </b>

<dt>October 18, Lecture 13:
	<dd><b>Topics: </b> Multiple linear regression. Predicting with multiple predictors. Least squares with multiple predictors. Why multiple linear regression is different from combining several simple linear regressions. Why regression coefficients change when we change the predictors in our model: finding the area of a noisy rectangle.
	<dd><b>Sections: </b>

<dt>October 22, Lecture 14:
	<dd><b>Topics: </b>Stories we tell: the multiple linear regression with Gaussian noise (MLRGN) model. Assumption-checking for multiple linear regression: the same, but different. Properties of the ordinary least squares estimators under the MLRGN model.
	<dd><b>Sections: </b>

<dt>October 25, Lecture 15:
	<dd><b>Topics: </b>What makes an estimate significant? Statistical versus practical significance: redux. Confidence curves for the coefficients of a multiple linear regression model under the MLRGN model. Confidence curves for the coefficients of a multiple linear regression model via bootstrapping. Adjusting for multiple comparisons.
	<dd><b>Sections: </b>

<dt>October 29, Lecture 16:
	<dd><b>Topics: </b>Dealing with non-linearity. Adding a polynomial term. Dealing with categorical predictors by adding "dummy" variables. Interpreting the coefficients on categorical variables.
	<dd><b>Sections: </b>

<dt>November 1, Lecture 17:
	<dd><b>Topics: </b>Multicollinearity: what it is and why it's a problem. Identifying collinearity from pairwise plots of the predictors. Dropping predictors to remove collinearity. Ridge regression for handling multicollinearity.
	<dd><b>Sections: </b>

<dt>November 5, Lecture 18:
	<dd><b>Topics: </b>Testing and confidence sets for multiple coefficients. Testing individual coefficients in the context of a larger model. Testing for groups of coefficients in the context of a larger model. Testing all the slopes in the context of a larger model. Confidence rectangles via multiple comparisons. Confidence ellipsoids.
	<dd><b>Sections: </b>

<dt>November 8, Lecture 19:
	<dd><b>Topics: </b>Interactions. Interactions in a linear model. Interactions between numerical and categorical variables.
	<dd><b>Sections: </b>

<dt>November 12, Lecture 20:
	<dd><b>Topics: </b>Influential points and outliers. Influence of a data point on the ordinary least squares estimates. Detecting outliers. Dealing with outliers and influential points: deletion and robust regression.
	<dd><b>Sections: </b>

<dt>November 15, Lecture 21:
	<dd><b>Topics: </b>Lab 2.
	<dd><b>Sections: </b>

<dt>November 19, Lecture 22:
	<dd><b>Topics: </b>Exam 2.
	<dd><b>Sections: </b>

<dt>November 22, Lecture 23:
	<dd><b>Topics: </b>Model selection. Who wore it best? Silly things you may see people do. Modern approaches to model selection. The perils of inference after model selection.
	<dd><b>Sections: </b>

<dt>November 26, Lecture 24:
	<dd><b>Topics: </b>Variable selection as a special case of model selection. Finding important variables. What do we mean by "important". Please leave P out of this. Cross-validation for variable selection. All subsets selection. Stepwise regression. The perils of inference after model selection, redux.
	<dd><b>Sections: </b>

<dt>December 3, Lecture 25:
	<dd><b>Topics: </b>When the noise misbehaves: heteroskedasticity and non-constant noise variance. Weighted least squares. Weighted least squares in R.
	<dd><b>Sections: </b>

<dt>December 6, Lecture 26:
	<dd><b>Topics: </b>The world is neither linear, nor quadratic, nor... Nonparametric regression: it's regression, Jim, but not as we know it. Regression as local smoothing. Linear regression as the simplest local smoother. Nearest neighbor regression. Kernel regression. Tuning parameters and their selection. R packages for nonparametric regression.
	<dd><b>Sections: </b>

</dl>